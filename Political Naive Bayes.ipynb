{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes on Political Text\n",
    "\n",
    "In this notebook we use Naive Bayes to explore and classify political data. See the `README.md` for full details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Feel free to include your text patterns functions\n",
    "#from text_functions_solutions import clean_tokenize, get_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (3.8.1)\n",
      "Requirement already satisfied: joblib in /Users/georgegarcia/Library/Python/3.9/lib/python/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /Users/georgegarcia/Library/Python/3.9/lib/python/site-packages (from nltk) (4.61.2)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nltk) (8.1.3)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 23.0 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install --upgrade nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/georgegarcia/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/georgegarcia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "convention_db = sqlite3.connect(\"2020_Conventions.db\")\n",
    "convention_cur = convention_db.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Exploratory Naive Bayes\n",
    "\n",
    "We'll first build a NB model on the convention data itself, as a way to understand what words distinguish between the two parties. This is analogous to what we did in the \"Comparing Groups\" class work. First, pull in the text \n",
    "for each party and prepare it for use in Naive Bayes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "convention_data = []\n",
    "\n",
    "# fill this list up with items that are themselves lists. The \n",
    "# first element in the sublist should be the cleaned and tokenized\n",
    "# text in a single string. The second element should be the party. \n",
    "\n",
    "query_results = convention_cur.execute(\n",
    "                            '''CREATE TABLE IF NOT EXISTS conventions (\n",
    "                              text TEXT,\n",
    "                              party TEXT\n",
    "                            )\n",
    "                            ''')\n",
    "# Function for cleaning and tokenizing text\n",
    "def process_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    cleaned_text = \" \".join(tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "# Example text and party\n",
    "text = \"This is some example text.\"\n",
    "party = \"Democratic\"\n",
    "\n",
    "# Clean and tokenize text\n",
    "cleaned_text = process_text(text)\n",
    "\n",
    "# Insert the cleaned and tokenized text and party into the database\n",
    "convention_cur.execute(\"\"\"\n",
    "INSERT INTO conventions (text, party)\n",
    "VALUES (?, ?)\n",
    "\"\"\", (cleaned_text, party))\n",
    "\n",
    "# Commit the changes to the database\n",
    "convention_db.commit()\n",
    "\n",
    "convention_data = []\n",
    "query_results = convention_cur.execute(\"SELECT text, party FROM conventions\")\n",
    "for row in query_results :\n",
    "    convention_data.append([row[0], row[1]])# store the results in convention_data\n",
    "    pass # remove this\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some random entries and see if they look right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Virginia.', 'Democratic'],\n",
       " ['My journey to a religious life was not a traditional route if there is such a thing. In 1978 as a medical school student at Georgetown University, I joined the army to help pay for my tuition and ended up devoting 29 years to the military, serving as a doctor and a surgeon in places like Afghanistan and Egypt’s Sinai Peninsula. After much prayer and contemplation I entered my religious order in 2002, working to serve the poor and the sick in Haiti, Sudan, Kenya, Iraq and in Washington, D.C. Humility is at the foundation of our order which makes it very difficult to talk about myself but I can speak about my experience working for those fleeing, war-torn and impoverished countries all around the world. Those refugees all share a common experience. They have been all marginalized, viewed as insignificant, powerless and voiceless. And while we tend to think of the marginalized as living beyond our borders, the truth is the largest marginalized group in the world can be found here in the United States, they are the unborn.',\n",
       "  'Republican'],\n",
       " ['That’s right? Dr.', 'Democratic'],\n",
       " ['The American spirit reflects the oldest and most important virtues, self-sacrifice, courage, tolerance, love of country, grace and passion for human achievement. We can decide right now, that American greatness will not be rejected nor squandered, because the American founding was grounded in individual liberty. So, it will be our future. But if we are to rediscover our strength, then it must be an endeavor undertaken by each and one of us. We must become the heroes that we so admire. America was built by them and our future will be protected by them. It will be protected by you. So, God bless America.',\n",
       "  'Republican'],\n",
       " ['But someone else had our back, our president. When the winds had finished raging and the cleanup had only begun, he showed up. Now you might not know that because the national media didn’t report it, but the Trump administration was here in full force. The day after the storm, the president called to assure me that we have the full backing of the federal government and later that week, Vice President Pence came to Iowa to again assure us that the president and his administration were behind us. With the help of the Trump administration, we quickly received a major disaster declaration that will help Iowans get back on their feet. The president, he cut through the bureaucracy to do what needed to be done and to do it quickly.',\n",
       "  'Republican'],\n",
       " ['When Democrats called our work a token effort and walked out of the room during negotiations, because they wanted the issue more than they wanted a solution. Do we want a society that breeds success or a culture that cancels everything it even slightly disagrees with? I know where I stand because you see, I am living my mother’s American dream. My parents divorced when I was seven years old and we moved in with my grandparents into a two bedroom home, with me, my mom and my brother sharing a room and one bed. My mom worked 16 hours a day to keep food on the table and a roof over our heads. She knew that if we could find the opportunity, bigger things would come. I thought I had to use football to succeed in life and my focus on academics faded away. My freshman year, I failed out, I failed four subjects, Spanish, English, world geography, and even civics. Sen.',\n",
       "  'Republican'],\n",
       " ['“Now I’m going to ask you one more time, will you marry me?” And she goes like this, “Okay.” She put us back together. She gave me back my life. She gave us back a family. We are raised with the same values. Dr.',\n",
       "  'Democratic'],\n",
       " ['Thank you so much. Good evening America, and welcome to the fourth night of the 2020 Democratic National Convention, Uniting America. Okay, these last few nights have been going so well, we decided to add a fifth night where we will just play Michelle Obama’s speech on a loop. I first met Joe Biden when I was doing my show Veep. I played the Vice President and he was in fact, the Vice President, and we hit it off immediately. Soon after I was asked to be on the cover of a magazine. Remember those? And I was so excited. It was like, “Oh, what’s it going to be People or Vogue or Rolling Stone?” Well, it turns out it was for Arrive, the official onboard magazine of Amtrak, which nobody ever reads, even though it’s free.',\n",
       "  'Democratic'],\n",
       " ['After the end of the Cold War, Democrats and Republicans in Washington bought into the illusion that the whole world would start to resemble America. So, they started to pursue unlimited globalization. They welcome China into the world trade organization. They engaged in nation building in Afghanistan and tried to export democracy to Iraq. They signed a nuclear deal with Iran and a global climate agreement in Paris, but they didn’t ground any of it in the interest of the average American. So, for decades, while Washington politicians built a global system, American wages stagnated. Our great cities and industries were hollowed out. Entire communities were devastated and our manufacturing plants were shipped off to China. That’s what happened when Washington stopped being the capital of the United States and started being the capital of the world. As US Ambassador to Germany, I had a front row seat to Donald Trump’s America first foreign policy.',\n",
       "  'Republican'],\n",
       " ['We are a people in a quandary about the present. We are a people in search of our future. We are attempting to fulfill our national purpose to create and sustain a society in which all of us are equal.',\n",
       "  'Democratic']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices(convention_data,k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that looks good, we now need to make our function to turn these into features. In my solution, I wanted to keep the number of features reasonable, so I only used words that occur at least `word_cutoff` times. Here's the code to test that if you want it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a word cutoff of 5, we have 2891 as features in the model.\n"
     ]
    }
   ],
   "source": [
    "word_cutoff = 5\n",
    "\n",
    "tokens = [w for t, p in convention_data for w in t.split()]\n",
    "\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "\n",
    "feature_words = set()\n",
    "\n",
    "for word, count in word_dist.items() :\n",
    "    if count > word_cutoff :\n",
    "        feature_words.add(word)\n",
    "        \n",
    "print(f\"With a word cutoff of {word_cutoff}, we have {len(feature_words)} as features in the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_features(text,fw) :\n",
    "    \"\"\"Given some text, this returns a dictionary holding the\n",
    "       feature words.\n",
    "       \n",
    "       Args: \n",
    "            * text: a piece of text in a continuous string. Assumes\n",
    "            text has been cleaned and case folded.\n",
    "            * fw: the *feature words* that we're considering. A word \n",
    "            in `text` must be in fw in order to be returned. This \n",
    "            prevents us from considering very rarely occurring words.\n",
    "        \n",
    "       Returns: \n",
    "            A dictionary with the words in `text` that appear in `fw`. \n",
    "            Words are only counted once. \n",
    "            If `text` were \"quick quick brown fox\" and `fw` = {'quick','fox','jumps'},\n",
    "            then this would return a dictionary of \n",
    "            {'quick' : True,\n",
    "             'fox' :    True}\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # Your code here\n",
    "    tokens = set(word_tokenize(text))\n",
    "    dict = {word: True for word in tokens if word in fw}\n",
    "    return dict\n",
    "    ret_dict = dict()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-be3e6a1914ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m assert(conv_features(\"donald is the president\",feature_words)==\n\u001b[0m\u001b[1;32m      3\u001b[0m        {'donald':True,'president':True})\n\u001b[1;32m      4\u001b[0m assert(conv_features(\"people are american in america\",feature_words)==\n\u001b[1;32m      5\u001b[0m                      {'america':True,'american':True,\"people\":True})\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert(len(feature_words)>0)\n",
    "assert(conv_features(\"donald is the president\",feature_words)==\n",
    "       {'donald':True,'president':True})\n",
    "assert(conv_features(\"people are american in america\",feature_words)==\n",
    "                     {'america':True,'american':True,\"people\":True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build our feature set. Out of curiosity I did a train/test split to see how accurate the classifier was, but we don't strictly need to since this analysis is exploratory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(conv_features(text,feature_words), party) for (text, party) in convention_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20220507)\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "test_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.436\n"
     ]
    }
   ],
   "source": [
    "test_set, train_set = featuresets[:test_size], featuresets[test_size:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                 radical = True           Republ : Democr =     37.2 : 1.0\n",
      "                   media = True           Republ : Democr =     36.1 : 1.0\n",
      "             enforcement = True           Republ : Democr =     33.0 : 1.0\n",
      "                   votes = True           Democr : Republ =     24.0 : 1.0\n",
      "               greatness = True           Republ : Democr =     19.4 : 1.0\n",
      "                   China = True           Republ : Democr =     18.6 : 1.0\n",
      "                   taxes = True           Republ : Democr =     16.7 : 1.0\n",
      "                 destroy = True           Republ : Democr =     16.2 : 1.0\n",
      "                 climate = True           Democr : Republ =     15.5 : 1.0\n",
      "                supports = True           Republ : Democr =     15.2 : 1.0\n",
      "                 company = True           Republ : Democr =     13.1 : 1.0\n",
      "                    Mike = True           Republ : Democr =     12.9 : 1.0\n",
      "                    Lady = True           Republ : Democr =     12.3 : 1.0\n",
      "                    flag = True           Republ : Democr =     12.3 : 1.0\n",
      "                 amazing = True           Republ : Democr =     12.0 : 1.0\n",
      "                 Abraham = True           Republ : Democr =     11.0 : 1.0\n",
      "                    Iran = True           Republ : Democr =     11.0 : 1.0\n",
      "                  Second = True           Republ : Democr =     11.0 : 1.0\n",
      "                  defund = True           Republ : Democr =     11.0 : 1.0\n",
      "               destroyed = True           Republ : Democr =     11.0 : 1.0\n",
      "                 pledged = True           Republ : Democr =     11.0 : 1.0\n",
      "                  record = True           Republ : Democr =     11.0 : 1.0\n",
      "                 violent = True           Republ : Democr =     11.0 : 1.0\n",
      "                 Chinese = True           Republ : Democr =     10.0 : 1.0\n",
      "                    cuts = True           Republ : Democr =     10.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a little prose here about what you see in the classifier. Anything odd or interesting?\n",
    "\n",
    "### The things that stood out to me was that word radical was used more by republican than democrats, the word slady was used more by republican. I was woundering how much was this influenced by donald trump \n",
    "\n",
    "_Your observations to come._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Classifying Congressional Tweets\n",
    "\n",
    "In this part we apply the classifer we just built to a set of tweets by people running for congress\n",
    "in 2018. These tweets are stored in the database `congressional_data.db`. That DB is funky, so I'll\n",
    "give you the query I used to pull out the tweets. Note that this DB has some big tables and \n",
    "is unindexed, so the query takes a minute or two to run on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "cong_db = sqlite3.connect(\"congressional_data.db\")\n",
    "cong_cur = cong_db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cong_cur.execute(\n",
    "        '''\n",
    "           SELECT DISTINCT \n",
    "                  cd.candidate, \n",
    "                  cd.party,\n",
    "                  tw.tweet_text\n",
    "           FROM candidate_data cd \n",
    "           INNER JOIN tweets tw ON cd.twitter_handle = tw.handle \n",
    "               AND cd.candidate == tw.candidate \n",
    "               AND cd.district == tw.district\n",
    "           WHERE cd.party in ('Republican','Democratic') \n",
    "               AND tw.tweet_text NOT LIKE '%RT%'\n",
    "        ''')\n",
    "\n",
    "results = list(results) # Just to store it, since the query is time consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = []\n",
    "\n",
    "# Now fill up tweet_data with sublists like we did on the convention speeches.\n",
    "# Note that this may take a bit of time, since we have a lot of tweets.\n",
    "query_results = cong_cur.execute(\n",
    "                            '''\n",
    "                            CREATE TABLE IF NOT EXISTS results (\n",
    "                              tweet_text TEXT,\n",
    "                              party TEXT\n",
    "                            )\n",
    "                            ''')\n",
    "\n",
    "def process_text(text):\n",
    "    tokens = word_tokenize(tweet_text)\n",
    "    cleaned_text = \" \".join(tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "# Example tweet and party\n",
    "tweet_text = \"This is some example text.\"\n",
    "party = \"Democratic\"\n",
    "\n",
    "# Clean and tokenize text\n",
    "cleaned_text = process_text(tweet_text)\n",
    "\n",
    "# Insert the cleaned and tokenized text and party into the database\n",
    "cong_cur.execute(\"\"\"\n",
    "INSERT INTO results (tweet_text, party)\n",
    "VALUES (?, ?)\n",
    "\"\"\", (cleaned_text, party))\n",
    "\n",
    "# Commit the changes to the database\n",
    "cong_db.commit()\n",
    "\n",
    "teet_data = []\n",
    "query_results = cong_cur.execute(\"SELECT tweet_text, party FROM results\")\n",
    "for row in query_results :\n",
    "    tweet_data.append([row[0], row[1]])# store the results in convention_data\n",
    "    pass # remove this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of tweets here. Let's take a random sample and see how our classifer does. I'm guessing it won't be too great given the performance on the convention speeches..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20201014)\n",
    "\n",
    "tweet_data_sample = random.choices(tweet_data,k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This is some example text .', 'Democratic'],\n",
       " ['This is some example text .', 'Democratic'],\n",
       " ['This is some example text .', 'Democratic'],\n",
       " ['This is some example text .', 'Democratic'],\n",
       " ['This is some example text .', 'Democratic'],\n",
       " ['This is some example text .', 'Democratic'],\n",
       " ['This is some example text .', 'Democratic'],\n",
       " ['This is some example text .', 'Democratic'],\n",
       " ['This is some example text .', 'Democratic'],\n",
       " ['This is some example text .', 'Democratic']]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_data_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's our (cleaned) tweet: This is some example text .\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: This is some example text .\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: This is some example text .\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: This is some example text .\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: This is some example text .\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: This is some example text .\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: This is some example text .\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: This is some example text .\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: This is some example text .\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: This is some example text .\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for tweet, party in tweet_data_sample :\n",
    "    estimated_party = classifier.classify(features)\n",
    "    results[party][estimated_party] += 1\n",
    "    # Fill in the right-hand side above with code that estimates the actual party\n",
    "    \n",
    "    print(f\"Here's our (cleaned) tweet: {tweet}\")\n",
    "    print(f\"Actual party is {party} and our classifer says {estimated_party}.\")\n",
    "    print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've looked at it some, let's score a bunch and see how we're doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of counts by actual party and estimated party. \n",
    "# first key is actual, second is estimated\n",
    "parties = ['Republican','Democratic']\n",
    "results = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for p in parties :\n",
    "    for p1 in parties :\n",
    "        results[p][p1] = 0\n",
    "\n",
    "\n",
    "num_to_score = 10000\n",
    "random.shuffle(tweet_data)\n",
    "\n",
    "for idx, tp in enumerate(tweet_data) :\n",
    "    tweet, party = tp    \n",
    "    # Now do the same thing as above, but we store the results rather\n",
    "    # than printing them. \n",
    "    features = conv_features(tweet, feature_words)\n",
    "    for p in parties:\n",
    "        if classifier.classify(features) == p:\n",
    "            results[party][p] += 1\n",
    "    if idx == num_to_score:\n",
    "        break\n",
    "   \n",
    "    # get the estimated party\n",
    "    estimated_party = classifier.classify(features)\n",
    "    \n",
    "    results[party][estimated_party] += 1\n",
    "    \n",
    "    if idx > num_to_score : \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'Republican': defaultdict(int,\n",
       "                         {'Republican': 0, 'Democratic': 0}),\n",
       "             'Democratic': defaultdict(int,\n",
       "                         {'Republican': 0, 'Democratic': 28})})"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflections\n",
    "\n",
    "_the cod eiwrote is only looking at the democratic party and is able to predict it pretty well but it is missing the republican party, it might have to di with my example text not having rthe republican_ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
